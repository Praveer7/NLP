{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-1 NLP: A PRIMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural Language Processing** \n",
    "\n",
    "It is an area of computer science that deals with methods to analyze, model, and understand human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow of this Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overview of applications of NLP in real-world scenarios\n",
    "\n",
    "- Various tasks that form basis of building different NLP applications\n",
    "\n",
    "- Understanding of language from NLP perspective and why NLP is difficult\n",
    "\n",
    "- Overview of heuristics, ML & DL\n",
    "\n",
    "- Introduction to a few commonly used algorithms in NLP\n",
    "\n",
    "- Walkthrough of an NLP application\n",
    "\n",
    "- Overview of the rest of topics in this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization of Chapters in terms of NLP tasks and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Core Tasks** (Chapters 3-7)\n",
    "\n",
    "\n",
    "- Text Classification\n",
    "- Information Extraction\n",
    "- Conversational Agent\n",
    "- Information Retrieval\n",
    "- Question Answering Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **General Applications** (Chapters 4-7)\n",
    "\n",
    "\n",
    "\n",
    "- Spam Classification\n",
    "- Calendar Event Extraction\n",
    "- PersonalAssistants\n",
    "- Search Engines\n",
    "- Jeopardy !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Industry Specific** (Chapters 8-10)\n",
    "\n",
    "\n",
    "- Social Media Analysis\n",
    "- Retail Catalog Extraction\n",
    "- Health Records Analysis\n",
    "- Financial Analysis\n",
    "- Legal Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in the Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Mail Applications** (Ch-4 & 5)\n",
    "\n",
    "- GMail, Outlook, etc.\n",
    "- provide features like spam, classification, priority inbox, calendar event extraction, auto complete etc.\n",
    "\n",
    "**Voice-based Assistants** (Ch-6)\n",
    "\n",
    "- Apple Siri, Google Assistant, Microsoft Cortana, Amazon Alexa\n",
    "- rely on a range of NLP techniques to interact with user, understand user commands and respond accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modern Search Engines** (Ch-7)\n",
    "\n",
    "\n",
    "- Google and Bing\n",
    "- use NLP heavily for various subtasks like query understanding, query expansion, question answering, information retrieval and ranking and grouping of the results\n",
    "\n",
    "\n",
    "**Machine Translation Services** (Ch-7)\n",
    "\n",
    "\n",
    "- Google Translate, Bing, Microsoft Translator and AMazon Translate\n",
    "- direct applications of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Organizations analyze their **social media feeds** to understand voice of their customers (Ch-8)\n",
    "\n",
    "- NLP is widely used to solve use cases on **e-commerce platforms** (Amazon) like extracting relevant information from product descriptions, understanding user reviews, etc. (Ch-9)\n",
    "\n",
    "- To solve use cases in domains such as **healthcare, finance and law** (Ch-10)\n",
    "\n",
    "- Companies like Arria use NLP techniques to **automatically generate reports for various domains** (weather forecasting, financial services, etc.)\n",
    "\n",
    "- NLP forms backbone of **spelling- and grammar-correction tools** (Grammarly, spell check in Microsoft Word & Google Docs)\n",
    "\n",
    "- In popular quiz show: ***Jeopardy !***, **Watson AI** won the first prize. It was built using NLP techniques.\n",
    "\n",
    "- NLP is used in a range of **learning and assessment tools and technologies** (automated scoring in GRE, plagiarism detection like Turnitin, intelligent tutoring systems, language learning apps like Duolingo)\n",
    "\n",
    "- NLP is used to built **large knowledge bases** (Google Knowledge Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental tasks that appear frequently across various NLP projects:\n",
    "\n",
    "i. **Language Modeling**\n",
    "\n",
    "\n",
    "- predicting what the next word in a sentence will be based on history of previous words\n",
    "- Goal -> to learn probability of a sequence of words appearing in a given language\n",
    "- Uses -> speech recognition, optical character recognition, handwriting recognition, machine translation & spelling correction\n",
    "\n",
    "\n",
    "\n",
    "ii. **Text Classification**\n",
    "\n",
    "\n",
    "- task of bucketing the text into a known set of categories based on its content\n",
    "- most popular task in NLP\n",
    "- Uses -> email spam identification, machine translation & spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. **Information Extraction**\n",
    "\n",
    "\n",
    "- task of extracting relevant information from text\n",
    "- Uses -> calendar event extraction from emails, extracting names of people mentioned in a social media post, etc.\n",
    "\n",
    "\n",
    "iv. **Information Retrieval**\n",
    "\n",
    "\n",
    "- task of finding documents relevant to a user query from a large collection, e.g: Google search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. **Conversational Agent**\n",
    "\n",
    "\n",
    "- task of building dialogue systems that can converse in human languages, e.g: Alexa, Siri, etc.\n",
    "\n",
    "\n",
    "vi. **Text Summarization**\n",
    "\n",
    "\n",
    "- task of creating short summaries of longer documents while retaining core content and preserving overall meaning of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vii. **Question Answering**\n",
    "\n",
    "\n",
    "- task of building a system that can automatically answer questions posed in natural language\n",
    "\n",
    "\n",
    "viii. **Machine Translation**\n",
    "\n",
    "\n",
    "- task of converting a piece of text from one language to another, e.g: Google Translate\n",
    "\n",
    "\n",
    "\n",
    "ix. **Topic Modeling**\n",
    "\n",
    "\n",
    "- task of uncovering the topical structure of a large collection of documents\n",
    "- a common text mining tool used in domains like literature, bioinformatics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language from NLP Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Language** is a structured system of communication that involves complex combinations of its constituent components like charcters, words, sentences, etc.\n",
    "\n",
    "\n",
    "\n",
    "- **Linguistics** is the systematic study of language.\n",
    "\n",
    "\n",
    "\n",
    "- To study NLP, it is important to understand some concepts of linguistics.\n",
    "\n",
    "\n",
    "\n",
    "- Human language can be thought of as composed of four major building blocks:\n",
    "\n",
    "\n",
    ">   1. **Context** (meaning) : Applications -> Summarization, Topic Modeling, Sentiment Analysis \n",
    ">   2. **Syntax** (phrases & sentences) : Applications -> Parsing, Entity Extraction, Relation Extraction\n",
    ">   3. **Morphemes & Lexemes** (words) : Applications -> Tokenization, Word Embeddings, POS Tagging\n",
    ">   4. **Phonemes** (speech & sounds) : Applications -> Speech to text, Speaker Identification, Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHONEMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Phonemes** are the smallest units of sound in a language\n",
    "- they don't have any meaning of themselves\n",
    "- they can induce meaning when uttered in combination with other phonemes\n",
    "- Standard English has 44 phonemes (single letters / combination of letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- **Consonant phonemes** -> /b/ - bat, /s/-sun, /k/-cat, /sh/-shop, /p/-pen, /ng/-ring\n",
    "- **Vowel phonemes** -> /a/-ant, /oi/-coin, /e/-egg, /ear/-dear, /oa/-boat, /ow/-cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MORPHEMES & LEXEMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **Morpheme** is the smallest unit of language that has a meaning\n",
    "- It is formed by a combination of phonemes.\n",
    "- Not all morphemes are words.\n",
    "- All prefixes and suffixes are morphemes, e.g: in word '`multimedia`, `multi-` is a morpheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:  \n",
    "> unbreakable => un + break + able  \n",
    "> cats => cat + s  \n",
    "  (*these morphemes are just constituents of full words*)\n",
    "  \n",
    "> tumbling => tumble + ing  \n",
    "> unreliability => un + rely + able + ity  \n",
    "(*there is some variation when words are broken into morphemes*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lexemes** are strutural variations of morphemes related to one another by meaning, e.g: '`run` and `running` belong to same lexeme form\n",
    "\n",
    "\n",
    "- **Morphological Analysis** is a foundational block of many NLP tasks such as tokenization, stemming, learning word embedings & POS tagging.\n",
    "- It analyses the structure of word by studying its morphemes and lexemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYNTAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Syntax** is a set of rules to construct grammatically correct sentences out of words and phrases in a language.\n",
    "- In linguistics, there are many ways to represent syntactic structure.\n",
    "- A common example => Parse Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,  \n",
    "**N** = Noun  \n",
    "**V** = Verb  \n",
    "**P** = Preposition  \n",
    "**NP** = Noun Phrase  \n",
    "**VP** = Verb Phrase  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **Parse Tree** has a hierarchical structure of language => words at the lowest level followed by Parts-of-Speech tags followed by phrases, and ending with sentence at the highest level\n",
    "- **Parsing** is the NLP task that constrcuts such trees automatically.\n",
    "- On this knowledge of parsing, other NLP tasks can be buils => such as Entity extraction and Relation extraction (Ch-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context** is how various parts in a language come together to convey a particular meaning.\n",
    "- It includes:\n",
    "> - long-term references\n",
    "> - word knowledge\n",
    "> - common sense\n",
    "> - literal meaning of words and phrases\n",
    "- Meaning of a sentence can change based on context\n",
    "- Context can be of two types based on :\n",
    "\n",
    "> 1. Semantics: direct meaning of the words and sentences without external context\n",
    "> 2. Pragmatics: adds world knowledge & external context of the conversation to enable us to infer implied meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is NLP Challenging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two characteristics of human language that make NLP a demanding area to work in:-\n",
    "- Ambiguity\n",
    "- Creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMBIGUITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Ambiguity** means uncertainty of meaning\n",
    "- e.g: \"I made her duck\"  \n",
    "\n",
    "> 1st possible meaning:- I cooked a duck for her.  \n",
    "> 2nd possible meaning:- I made her bend down to avoid an object.\n",
    "\n",
    "- Which of the above two meaning applies depends on the context in which sentence appears.\n",
    "\n",
    "> Possible context of 1st:- story about a mother and a child.  \n",
    "> Possible context of 2nd:- a book about sports\n",
    "\n",
    "- Ambiguity increases even more in case of figurative language, i.e. idioms\n",
    "\n",
    "- Examples of ambiguity in language\n",
    "\n",
    "> The man couldn't lift his son because he was so weak => ***Who was weak?***  \n",
    "> Joan made sure to thank Susan for all the help she had given => ***Who had given help?***  \n",
    "> John promised Bill to leave, so an hour later he left => ***Who left an hour later?***\n",
    "\n",
    "- The above examples are easily disambiguated by a human but are not solvable using NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMMON KNOWLEDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is the set of all facts that most humans are aware of.\n",
    "\n",
    "\n",
    "- e.g: \"`Man bit dog`\" & \"`Dog bit man`\"\n",
    "\n",
    "\n",
    "- We know that the 1st is unlikely to happen and the 2nd is very possible. This common knowledge was not mentioned in eaither of the two sentences.\n",
    "\n",
    "\n",
    "- The computer would find it very difficult to differentiate between the two sentences as it lacks common knowledge.\n",
    "\n",
    "\n",
    "- One of the key challenges of NLP => \"***How to encode all the things that are common knowledge to humans in a computational model.***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATIVITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language is not just rule driven => it also has a creativity aspec to it.\n",
    "\n",
    "\n",
    "\n",
    "- Various styles, dialects, genres, variations used in language, poems, etc.\n",
    "\n",
    "\n",
    "\n",
    "- Making machines understand creativity is a hard problem not just in NLP, but AI in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIVERSITY ACROSS LANGUAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Porting an NLP solution from one language to another is hard.\n",
    "\n",
    "\n",
    "- A solution that works for one language might not work at all for another language.\n",
    "\n",
    "\n",
    "- Two possible solutions:\n",
    "\n",
    "> 1. build a soluion that is language agnostic => this is conceptually very hard\n",
    "> 2. build separate solutions for each language => laborious and time intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All above issues make NLP a challenging, yet rewarding domain to work in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning, Deep Learning & NLP: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Artificial Intelligence(AI)** is a branch of computer science that aims to build systems that can perform tasks that require human intelligence.\n",
    "\n",
    "\n",
    "- **Machine Learning (ML)** is a branch of AI that deals with the development of algorithms that can learn to perform tasks automatically based on large number of examples, without requiring handcrafter rules.\n",
    "\n",
    "\n",
    "- **Deep Learning (DL)** is a branh of ML that is based on artificial neural network architectures.\n",
    "\n",
    "\n",
    "- ML, DL & NLP are all subfields within AI.\n",
    "\n",
    "\n",
    "- Early NLP applications were based on rules and heuristics.\n",
    "\n",
    "- In past few decades, NLP application development has been hevily influenced by methods from ML.\n",
    "\n",
    "- More recently, DL has also been frequently used to build NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Goal of ML*** => to learn to perform tasks based on examples (`training data`) without explicit instructions => this is done by creating numeric representation (features) of training data and using this representation to learn the patterns in those examples.\n",
    "\n",
    "\n",
    "- ML algorithms can be divided into three categories:-\n",
    "\n",
    "\n",
    "> 1. **Supervised Learning**: To learn the mapping function from input to output given a large number of examples in the form of input-output pairs (`training data`), e.g: spam e-mail classification\n",
    "> 2. **Unsupervised Learning**: A set of ML methods that aim to find hidden patterns in given input data without any reference output, e.g: Topic Modeling  \n",
    "> [**Semi-supervised Learning**: These techniques use both - a small labeled dataset and a large unlabeled dataset - to learn the task at hand.]\n",
    "> 3. **Reinforcement Learning**: It deals with methods to learn tasks via trial and error. It is characterized by absence of either labeled or unlabeled data in large quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different approaches used to solve NLP fall in three categories:-\n",
    "> 1. Heuristics\n",
    "> 2. Machine Learning\n",
    "> 3. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Heuristics-based NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Building rules for the task at hand.\n",
    "\n",
    "- **Limitations**\n",
    "    - developer were required to have some domain expertise\n",
    "    - such systems required resources like dictionaries and thesauruses\n",
    "    \n",
    "- **Example**: Lexicon-based sentiment analysis => it counts positive and negative words in text to deduce the sentiment of text (Ch-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Knowledge Bases built in NLP**\n",
    "\n",
    "\n",
    "1. **`WordNet`**: database of words and the semantic relationships between them.   \n",
    "e.g: *Synonyms*: words with similar meanings  \n",
    "*Hyponyms*: ***is-type-of*** relationships, e.g: baseball, sumo wrestling & tennis are hyponyms of sports  \n",
    "*Meronyms*: ***is-part-of*** relationships, e.g: hands & legs are meronyms of body  \n",
    "\n",
    "\n",
    "2. **`Open Mind Common Sense`**: Knowledge base in which common sense world knowledge has been incorporated\n",
    "\n",
    "\n",
    "Both above knowledge bases are lexical resources based on world-level knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rule-based Systems**: They go beyond words and can incorporate other forms of information too.\n",
    "\n",
    "\n",
    "1. **`Regex (Regular Expressions)`**\n",
    "\n",
    "    - Great tool for text analysis\n",
    "    - A set of characters or a pattern that is used to match and find substrings in text.\n",
    "    - e.g: To find all email IDs in a place of text, we use regex     \n",
    "    ([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2, 5})$  \n",
    "    \n",
    "    - A NLP software => ***Stanford Core NLP*** => it includes TokensRegex (framework for defining regex)\n",
    "    - **Regexes** are used for deterministic matches (it's either a match or ot's not)\n",
    "    - **Probabilistic Regexes** is a sub-branch that addresses this limitation by including a probability of a match. e.g: software libraries like ***pregex***\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`CFG (Content-Free Grammar)`**\n",
    "\n",
    "    - A type of formal grammar that is used to model natural languages.\n",
    "    - Invented by Prof. Noam Chomsky (renowned linguist and scientist)\n",
    "    - CFGs can be used to capture more complex and hierarchical information that a regex might not.\n",
    "    - **Early Parser** allows parsing all kinds of CFGs.\n",
    "    - **JAPE (Java Annoatation Patterns Engine)** can model more complex rules and grammar. It has features from both regexes as wel as CFGs.\n",
    "    - **GATE (General Architecture for Text Engg.)** is used for building text extraction for closed and well-defined domains where accuracy and completeness of coverage is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Importance of Rules & Heuristics**\n",
    "\n",
    "    - They help to quickly build the first version of the model and get a better understanding of problem at hand. (Ch-4 & 11)\n",
    "    - Can be useful as features for machine learning-based NLP systems.\n",
    "    - They are used to plug the gaps in the system (where statistical, ML or DL techniques will make mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised ML techniques => classification & regression => are hevily used for various NLP tasks.\n",
    "- **Example of classification**: to classify news articles into a set of news topics like sports and politics\n",
    "- **Example of regression**: to estimate price of a stock based on processing the social media discussion about that stock\n",
    "- Unsupervised ML techniques like clustering can be used to club together text documents.\n",
    "\n",
    "\n",
    "\n",
    "- 3 common steps of any ML approach to NLP:-  \n",
    "    i. extracting features from text  \n",
    "    ii. using the feature representation to learn a model (Ch-3)\n",
    "    iii. evaluating and improving the model (Ch-2)\n",
    "\n",
    "- Some commonly used supervised ML methods for NLP (2nd step) are:- Naive Bayes, Support Vector Machine, Hidden Markov Model and Conditional Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classic algorithm for classification tasks\n",
    "- Mainly relies on Bayes' Theorem\n",
    "- Using Bayes' Theorem, it calculates the probability of observing a class label given the set of features for the input data.\n",
    "- Important characteristic => it assumes each feature is independent of all other features.  \n",
    "e.g: In news classification task, we assume that domain-specific words (such as sports-specific or politics-specific) are not correlated to one another.\n",
    "- Naive Bayes is used as a starting algorithm for text classification => because it is simple to understand and very fast to train and run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another popular classificarion algorithm.\n",
    "- Goal of any classification problem => to learn a decision boundary that acts as a separation between different categories of text => this boundary can be linear or non-linear.\n",
    "- An SVM can learn both a linear and non-linear decision boundary to separate data points belonging to different classes.\n",
    "- **Strength of SVM**: Robustness to variation and noise in data\n",
    "- **Weakness of SVM**: Time taken to train and inability to scale when there are large amounts of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIDDEN MARKOV MODEL (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HMM is a statistical model.\n",
    "- It assumes that there is an underlying unobservable process with hidden states generates the data (we can only observe the data once it is generated).\n",
    "- An HMM tries to model the hidden states from this data.  \n",
    "e.g: HMMs are used for POS tagging of text data.   \n",
    "Here, the underlying unobservable process is => grammar and hidden states are => Parts-of-Speech (POS)\n",
    "- **`Markov` Assumption**: Each hidden state is dependent on the previous state(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONDITIONAL RANDOM FIELDS (CRF)   (Ch-5, 6 & 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another algorithm used for sequential data.\n",
    "- CRF essentially performs a classification task on each element in the sequence.\n",
    "- **Why CRFs are better?**: Because CRF takes the sequential input and the context of tags into consideration.\n",
    "- CRFs outperform HMMs for tasks such as POS Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In last few years, we have seen a huge surge in neural networks to deal with complex, unstructured data.\n",
    "- Following are a few popular deep neural networks architectures that have become the status quo in NLP:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECURRENT NEURAL NETWORKS (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language is inherently sequential.\n",
    "- A sentence in any language flows from one direction to another.\n",
    "- A model that can progressively read an input text from one end to another can be very useful for language understanding.\n",
    "- RNNs are specially designed to keep such sequential processing and learning in mind.\n",
    "- RNNs have **neural units** that are capable of remembering what they have processed so far.\n",
    "- This memory is **temporal**, an info is stored and updated with every time step as the RNN reads the next word in the input.\n",
    "- RNNs are used in NLP tasks like => Text classification, NER, machine translation, etc.\n",
    "- RNNs can also be used to generate text where the goal is to read the preceding text and predict the next word or next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LONG SHORT-TERM MEMORY (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Disadvantages of RNNs**: Forgetful memory => they can't remember longer contexts => so they don;t perform well when input text is long (which is usually the case).\n",
    "- LSTMs circumvent this problem by letting go of irrelevant contect and only remembering the part of context that is needed to solve the task at hand.\n",
    "- This relieves the load of remembering very long context in one vector representation.\n",
    "- **Gated Recurrent Units (GRUs)**: Another variant of RNNs that are used mostly in language generation.\n",
    "- Specific use of LSTMs in various NLP applications => Ch-4, 5, 6, 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVOLUTIONAL NEURAL NETWORK (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNNs are used heavily in computer vision tasks like image classification, video recognition, etc.\n",
    "- In NLP, CNNs have seen success in **text-classification tasks**.\n",
    "\n",
    "    - One can replace each word in a sentence with its corresponding word vector. All vectorsare of same size (d).\n",
    "    - They can be stacked one over another to form a matrix or 2D array of dimension n X d, where n is the number of words in sentence and d is size of word vectors.\n",
    "    - This matrix can be treated similar to an image and can be modeled by a CNN.\n",
    "\n",
    "- **Advantage of CNN**: Ability to look at a group of words tegether using a context window.\n",
    "- Uses of CNNs for NLP => Ch-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers have achieved state of the art in almost all NLP tasks.\n",
    "- They model the textual context but not in a sequential manner.\n",
    "\n",
    "\n",
    "- **Working Mechanism**:\n",
    "    - Given a word in the input, it prefers to look at all the words around it (known as `SELF-ATTENTION`) and represent each word w.r.t. its context.\n",
    "    - e.g: If context of the word `bank` talks about finance, then `bank` probably denotes a financial institution. On the other hand. if context mentions a river, then `bank` probably indicates a bank of the river.\n",
    "\n",
    "\n",
    "- Due to this higher representation capacity of transformers as compared to other deep networks, transformers are heavily used in NLP.\n",
    "\n",
    "\n",
    "- **Large Transformers**: Recently, used for transfer learning with smaller downstream tasks.\n",
    "\n",
    "\n",
    "- **Transfer Learning**: A technique in AI where the knowledge gained while solving one problem is applied to a different but related problem.\n",
    "\n",
    "\n",
    "- **Mechanism of large transformers**: \n",
    "\n",
    "    - To train very large transformer mode in an unsupervised manner (PRE-TRAINING) to predict a part of a sentence given the rest of the content so that it can encode the high-level nuances of language in it.\n",
    "    - These models are trained on more than 40 GB of textual data, scraped from the whole internet.\n",
    "    \n",
    "\n",
    "- **BERT (Bidirectional Encoder Representation from Transformers)**:\n",
    "\n",
    "    - An example of a large transformer, pre-trained on massive data and open sourced by Google.\n",
    "    - This pre-trained model is then fine-tuned on downstream NLP tasks, such as text-classification, entity extraction, question answering, etc.\n",
    "    - BERT works efficiently in transferring the knowledge for downstream tasks due to sheer amount of pre-trained knowledge.\n",
    "    - BERT and its applications => Ch-4, 6, 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOENCODERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An autoencoder is a different kind of network that is used mainly for learning ***compressed vector representation of the input***.\n",
    "- **Example**: To represent a text by a vector, we can learn a mapping function from the input text to the vector.\n",
    "\n",
    "    - To make this mapping function useful, we reconstruct input back from vector representation (Unsupervised learning)\n",
    "    - After training, we collect vector representation, which serves as an encoding of the input text as a dense vactor.\n",
    "    \n",
    "- Autoencoders are used to create feature representations needed for any downstream tasks.\n",
    "- Some variations of autoencoders like **LSTM autoencoders** can handle specific properties of sequential data like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Learning is not yet the Silver Bullet for NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overfitting on small datasets\n",
    "\n",
    "- DL models tend to have more parameters than traditional ML models.\n",
    "- Many times, in development phase, sufficient training data is not available to train a complex network.\n",
    "- In such cases, a simple model should be preferred over a DL model (Occam's Razor)\n",
    "- DL models overfit on small datasets. This leads to poor generalization capability, which in turn leads to poor performance in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Few-shot learning and synthetic data generation\n",
    "\n",
    "- **Few-shot learning**: learning from very few training examples\n",
    "- DL has made significant strides in few-shot learning and in models that can generate superior-quality images.\n",
    "- These advances have made it feasible to train DL-based vision models on small amounts of data.\n",
    "- Therefore, DL is widely adopted in solving problems in industrial settings.\n",
    "- We have not yet seen similar DL techniques be successfully developed for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Domain adaptation\n",
    "\n",
    "- If we utilize a large DL model trained on datasets from common domains and apply the trained model to a newer domain that is different from the common domains => it may yield poor performance => this is called **Loss in Generalization**\n",
    "- This shows that DL models are not always useful.\n",
    "- e.g: models trained on internet texts and product reviews will not work well when applied to domains such as law, social media or healthcare\n",
    "- We need specialized models to encode the domain knowledge, which could be as simple as domain-specific, rule-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Interpretable models\n",
    "\n",
    "- Most of the time, DL models work like a black box. So, controllability and interpretability is hard for DL models.\n",
    "- Businesses often demand more interpretable results that can be explained to the customer or end user => in such cases, traditional techniques might be more useful.\n",
    "- e.g: A Naive Bayes model for sentiment classification may explain the effect of strong positive and negative words on the final prediction of sentiment. Whie obtaining such insights from an LSTM-based classification model is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Common sense and world knowledge\n",
    "\n",
    "- Beyond syntax and semantics, language encompasses knowledge of the world around us.\n",
    "- Language for communication relies on logical reasoning and common sense regarding events of the world.\n",
    "- **Example of multistep reasoning**: \"If John walks out of the bedroom and goes to the garden, then John is not in the bedroom anymore and his current location is garden.\"  \n",
    "This requires multistep reasoning for a machine to identify events and understand their consequences.\n",
    "- Understanding such common sense and world knowledge that is inherent in language, is crucial for any DL model to perform wellon various language tasks.\n",
    "- Current DL models may perform well on standard benchmarks => but are still not capable of common sense understanding and logical reasoning.\n",
    "- These are some efforts to collect common sense events and logical rules (like 'if-then') => but they are not well integrated yet with ML or DL models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Cost\n",
    "\n",
    "\n",
    "- Building DL solutions for NLP tasks can be pretty expensive.\n",
    "- Cost is in terms of both => money and time\n",
    "- DL models are known as **DATA GUZZLERS** => they collect a large dataset and get it labelled\n",
    "- Training large DL models to achieve desired performance:\n",
    "    - increases development cycles\n",
    "    - results in heavy bills for specialized hardware (GPUs)\n",
    "- Deploying and maintaining DL models can be expensive both in terms of hardware requirements and effort\n",
    "- As these models are bulky, they may cause **LATENCY ISSUES** during inference time => they may not be useful in cases where low latency is a must.\n",
    "- One more reason => **TECHNICAL DEBT** arising from building and maintaining a heavy model => it is the cost of rework that arises from prioritizing speedy delivery over good design and implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. On-device employment\n",
    "\n",
    "- For many use cases => NLP solution needs to be deployed on an embedded device rather than in the cloud\n",
    "- e.g: a machine translation system that helps tourists speak the translated text even without the internet.\n",
    "- In such cases => the solution must work with limited memory and power => most DL solutions don't fit such constraints\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- In most industry projects => one or more of above mentioned (7) points play out => it leads to longer project cycles and higher costs (hardware, manpower) => the performance is sometimes comparable or even lower than ML methods => Thus, we get poor return on investment => it often causes NLP project to fail.\n",
    "- Thus, `DL is not always the go-to solution for industrial NLP applications`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
